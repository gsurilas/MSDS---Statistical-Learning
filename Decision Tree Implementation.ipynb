{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the implementation of the decision tree I found some Python code online to help me put it together, which the professor\n",
    "#said was allowed. I went through and annotated all the code to show that I understood it. I also coded my own entropy score\n",
    "#function in order to meet the homework requirements.\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "# Splits a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    #Initializes left and right lists for the split\n",
    "    left, right = list(), list()\n",
    "    #Separates dataset based on given value\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def entropy_score(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    \n",
    "    #Initialize entropy score\n",
    "    entropy = 0.0\n",
    "    #This section calculates the entropy score for each group in the split. Then sums the scores for a total\n",
    "    #entropy score of the chosen split\n",
    "    for group in groups:\n",
    "        size = float(len(group)) #Find the size of the current group\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        #Calculate entropy score for each group based on class values and then add for total entropy score\n",
    "        for class_val in classes:\n",
    "            #code selects row[-1] because it assumes that the response variable of a row is held in the last column\n",
    "            #Calculate proportion of how many rows belong to each class\n",
    "            p = [row[-1] for row in group].count(class_val) / size \n",
    "            #Avoid dividing by 0 in case of perfect split\n",
    "            if(p == 0):\n",
    "                continue\n",
    "            #Calculate entropy score for group \n",
    "            score += p*math.log(1/p) \n",
    "        entropy += score\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "    #Inspect the final column of the dataset(assumed to be Y response) and get the set of class values\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    #Initialize some values\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    #Go through each row of the dataset and create a test split. Evaluate the entropy score of that split\n",
    "    #Keep the entropy score of the best split and return its associated index, value and the groups it made\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            entropy = entropy_score(groups, class_values)\n",
    "            if entropy < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], entropy, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    #Creates a terminal node and returns the class value with the most outcomes in the node\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_size, depth+1)\n",
    " \n",
    "\n",
    "#Recursively builds the decision tree. First creates a root, then contintues to split on the root until max_depth or min_size\n",
    "#is reached\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split(train)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    " \n",
    "# Print a decision tree\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))\n",
    "        \n",
    "# Make a prediction with a decision tree\n",
    "#Takes a row from the dataset we're predicting for and follows the tree path according to the splitting rules\n",
    "#Returns the predicted value for the node\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "[X2 < 5.000]\n",
      " [X1 < 9.000]\n",
      "  [X8 < 9.000]\n",
      "   [X3 < 7.000]\n",
      "    [X7 < 8.000]\n",
      "     [X1 < 1.000]\n",
      "      [0.0]\n",
      "      [0.0]\n",
      "     [1.0]\n",
      "    [1.0]\n",
      "   [1.0]\n",
      "  [X1 < 10.000]\n",
      "   [1.0]\n",
      "   [X1 < 10.000]\n",
      "    [1.0]\n",
      "    [1.0]\n",
      " [X1 < 1.000]\n",
      "  [1.0]\n",
      "  [1.0]\n",
      "Decision Tree Accuracy rate: 0.9487554904831625\n",
      "Boosting Tree Accuracy Rate: 0.9809663250366032\n"
     ]
    }
   ],
   "source": [
    "#Question 1 Part 2 - Use Decision Tree on Breast dataset\n",
    "\n",
    "import pandas\n",
    "import sklearn\n",
    "import math\n",
    "import numpy\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import pandas\n",
    "import sklearn\n",
    "import math\n",
    "import numpy\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "breast_df = pandas.read_csv(\"C:\\\\Users\\\\George\\\\Documents\\\\Rutgers\\\\Statistical Learning\\\\Homework 1\\\\breast_final.csv\")\n",
    "#Remove last column which has missing values and remove ID Column\n",
    "breast_df = breast_df[breast_df.columns[1:]]\n",
    "#Remove NA from Column X6\n",
    "breast_df = breast_df.dropna(axis = 0)\n",
    "#Convert Y column into binary\n",
    "breast_df.Y.replace([2, 4], [0,1], inplace=True) \n",
    "\n",
    "#Convert breast dataframe into list of lists so it can work in Decision Tree code\n",
    "temp = breast_df.reset_index().values.tolist()\n",
    "breast = []\n",
    "for row in temp:\n",
    "    breast.append(row[1:])\n",
    "\n",
    "breast_tree = build_tree(breast,8,15)\n",
    "print(\"Decision Tree\")\n",
    "print_tree(breast_tree)\n",
    "\n",
    "#Get predictions with tree\n",
    "predictions = list()\n",
    "for row in breast:\n",
    "    prediction = predict(breast_tree, row)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "#Get true response values\n",
    "responses = list()\n",
    "for row in breast:\n",
    "    responses.append(row[-1])\n",
    "\n",
    "#Get decision tree accuracy rate\n",
    "correct = 0\n",
    "for i in range(len(responses)):\n",
    "    if(responses[i] == predictions[i]):\n",
    "        correct += 1\n",
    "\n",
    "print(\"Decision Tree Accuracy rate:\", correct/len(responses))\n",
    "\n",
    "#Run a Boosting method on the dataset\n",
    "boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n",
    "\n",
    "X = breast_df[breast_df.columns[0:9]]\n",
    "Y = breast_df[breast_df.columns[-1]].values.tolist()\n",
    "boost.fit(X,Y)\n",
    "preds = boost.predict(X)\n",
    "\n",
    "#Get Boosting Accuracy Rate\n",
    "correct = 0\n",
    "for i in range(len(preds)):\n",
    "    if(preds[i] == Y[i]):\n",
    "        correct += 1\n",
    "        \n",
    "print(\"Boosting Tree Accuracy Rate:\", correct/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
